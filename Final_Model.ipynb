{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLCuUaRpoW83",
        "outputId": "b6840ae9-7c4d-4b05-df49-54e13f6f13d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Working directory set to: /content/drive/.shortcut-targets-by-id/1avIasCHqris4iK4ri6WUTjhVPbSpzGPR/Amazon_ML_Challenge\n",
            "--------------------------------------------------\n",
            "All libraries for V4 model are ready.\n",
            "--------------------------------------------------\n",
            "Helper functions are defined.\n",
            "--------------------------------------------------\n",
            "Loading all data sources...\n",
            "Successfully loaded all text data and both train/test image features.\n",
            "--------------------------------------------------\n",
            "Step 5: Applying PCA to clean and compress image features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/decomposition/_pca.py:789: RuntimeWarning: invalid value encountered in divide\n",
            "  self.explained_variance_ratio_ = self.explained_variance_ / total_var\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PCA complete. Image features reduced from 2048 to 128 dimensions.\n",
            "Shape of new train PCA features: (75000, 128)\n",
            "Shape of new test PCA features: (75000, 128)\n",
            "--------------------------------------------------\n",
            "Step 6: Creating final datasets for training and testing...\n",
            "Final combined datasets created successfully.\n",
            "--------------------------------------------------\n",
            "Step 7: Training and validating the V4 PCA model...\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 10.454779 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 569322\n",
            "[LightGBM] [Info] Number of data points in the train set: 60000, number of used features: 8805\n",
            "[LightGBM] [Info] Start training from score 2.740904\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- V4 MODEL VALIDATION COMPLETE ---\n",
            "Previous Best Score (V2 Text Only): 0.5579\n",
            "Score for V3 (Text + Raw Images): 0.5624\n",
            "NEW V4 Score (Text + PCA Images): 0.5492\n",
            "--------------------------------------------------\n",
            "\n",
            "Step 8: Retraining on full data and generating final submission file...\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 13.104758 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 651340\n",
            "[LightGBM] [Info] Number of data points in the train set: 75000, number of used features: 9273\n",
            "[LightGBM] [Info] Start training from score 2.739217\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SUCCESS! New submission file 'pca_model_submission.csv' is ready in your Drive!\n"
          ]
        }
      ],
      "source": [
        "# --- STAGE 6: THE V4 COMEBACK MODEL (PCA + MULTIMODAL) ---\n",
        "# =========================================================\n",
        "# This is the final assembly script. It uses the pre-computed feature files.\n",
        "\n",
        "# --- Step 1: Connect to Drive and Set Paths ---\n",
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "project_path = '/content/drive/MyDrive/Amazon_ML_Challenge'\n",
        "# Set the working directory to our project folder\n",
        "os.chdir(project_path)\n",
        "print(f\"Working directory set to: {os.getcwd()}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "\n",
        "# --- Step 2: Import All Necessary Libraries ---\n",
        "from sklearn.decomposition import PCA\n",
        "from scipy.sparse import hstack\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "print(\"All libraries for V4 model are ready.\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "\n",
        "# --- Step 3: Define Helper Functions ---\n",
        "def smape(y_true, y_pred):\n",
        "    y_true_unlogged = np.expm1(y_true)\n",
        "    y_pred_unlogged = np.expm1(y_pred)\n",
        "    numerator = np.abs(y_pred_unlogged - y_true_unlogged)\n",
        "    denominator = (np.abs(y_true_unlogged) + np.abs(y_pred_unlogged)) / 2\n",
        "    ratio = np.where(denominator == 0, 0, numerator / denominator)\n",
        "    return np.mean(ratio)\n",
        "\n",
        "def feature_engineer(df):\n",
        "    df['text_length'] = df['catalog_content'].str.len()\n",
        "    def extract_ipq(text):\n",
        "        if not isinstance(text, str): return 1\n",
        "        text = text.lower()\n",
        "        patterns = [r'pack of (\\d+)', r'(\\d+)\\s*count', r'quantity\\s*[:]*\\s*(\\d+)', r'(\\d+)\\s*pack', r'set of (\\d+)', r'(\\d+)\\s*pk']\n",
        "        for pattern in patterns:\n",
        "            match = re.search(pattern, text)\n",
        "            if match: return int(match.group(1))\n",
        "        return 1\n",
        "    df['ipq'] = df['catalog_content'].apply(extract_ipq)\n",
        "    return df\n",
        "print(\"Helper functions are defined.\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "\n",
        "# --- Step 4: Load All Data and Features from Drive ---\n",
        "try:\n",
        "    print(\"Loading all data sources...\")\n",
        "    train_df = pd.read_csv('dataset/train.csv')\n",
        "    test_df = pd.read_csv('dataset/test.csv')\n",
        "    train_df['catalog_content'] = train_df['catalog_content'].fillna('')\n",
        "    test_df['catalog_content'] = test_df['catalog_content'].fillna('')\n",
        "\n",
        "    # Load our \"golden files\"\n",
        "    train_image_features = np.load('train_image_features.npy')\n",
        "    test_image_features = np.load('test_image_features.npy')\n",
        "    print(\"Successfully loaded all text data and both train/test image features.\")\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"ERROR: Could not find feature files. Please ensure they are in your Drive. Error: {e}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "\n",
        "# --- Step 5: Apply PCA to Image Features ---\n",
        "print(\"Step 5: Applying PCA to clean and compress image features...\")\n",
        "# We choose 128 components. This is a hyperparameter we could tune later.\n",
        "N_COMPONENTS = 128\n",
        "pca = PCA(n_components=N_COMPONENTS, random_state=42)\n",
        "\n",
        "# Fit PCA on the training images and transform both train and test images\n",
        "train_image_features_pca = pca.fit_transform(train_image_features)\n",
        "test_image_features_pca = pca.transform(test_image_features)\n",
        "\n",
        "print(f\"PCA complete. Image features reduced from 2048 to {N_COMPONENTS} dimensions.\")\n",
        "print(\"Shape of new train PCA features:\", train_image_features_pca.shape)\n",
        "print(\"Shape of new test PCA features:\", test_image_features_pca.shape)\n",
        "print(\"-\" * 50)\n",
        "\n",
        "\n",
        "# --- Step 6: Create Final Combined Datasets ---\n",
        "print(\"Step 6: Creating final datasets for training and testing...\")\n",
        "# Apply text feature engineering\n",
        "train_df = feature_engineer(train_df.copy())\n",
        "test_df = feature_engineer(test_df.copy())\n",
        "\n",
        "# Create TF-IDF features for the text\n",
        "tfidf = TfidfVectorizer(stop_words='english', max_features=10000)\n",
        "train_text_features = tfidf.fit_transform(train_df['catalog_content'])\n",
        "test_text_features = tfidf.transform(test_df['catalog_content'])\n",
        "\n",
        "# Get our simple numerical features\n",
        "train_numerical_features = train_df[['text_length', 'ipq']].values\n",
        "test_numerical_features = test_df[['text_length', 'ipq']].values\n",
        "\n",
        "# Combine everything into two final matrices: one for training, one for testing\n",
        "X_train_final = hstack([train_text_features, train_image_features_pca, train_numerical_features]).tocsr()\n",
        "X_test_final = hstack([test_text_features, test_image_features_pca, test_numerical_features]).tocsr()\n",
        "y_train_final = np.log1p(train_df['price'])\n",
        "print(\"Final combined datasets created successfully.\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# --- Step 7: Train and Validate the V4 Model ---\n",
        "print(\"Step 7: Training and validating the V4 PCA model...\")\n",
        "# We do one final validation split to get our score\n",
        "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train_final, y_train_final, test_size=0.2, random_state=42)\n",
        "lgbm_final = lgb.LGBMRegressor(random_state=42, n_estimators=500, learning_rate=0.05) # Added some basic tuning\n",
        "lgbm_final.fit(X_train_split, y_train_split)\n",
        "val_preds = lgbm_final.predict(X_val_split)\n",
        "final_smape = smape(y_val_split, val_preds)\n",
        "\n",
        "print(\"\\n--- V4 MODEL VALIDATION COMPLETE ---\")\n",
        "print(f\"Previous Best Score (V2 Text Only): 0.5579\")\n",
        "print(f\"Score for V3 (Text + Raw Images): 0.5624\")\n",
        "print(f\"NEW V4 Score (Text + PCA Images): {final_smape:.4f}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# --- Step 8: Generate Final Submission ---\n",
        "print(\"\\nStep 8: Retraining on full data and generating final submission file...\")\n",
        "# We retrain the model on ALL available training data for the best performance\n",
        "lgbm_final.fit(X_train_final, y_train_final)\n",
        "test_predictions_log = lgbm_final.predict(X_test_final)\n",
        "test_predictions = np.expm1(test_predictions_log)\n",
        "test_predictions[test_predictions < 0] = 0\n",
        "\n",
        "if not os.path.exists('submissions'): os.makedirs('submissions')\n",
        "submission_df = pd.DataFrame({'sample_id': test_df['sample_id'], 'price': test_predictions})\n",
        "submission_df.to_csv('submissions/pca_model_submission.csv', index=False)\n",
        "print(\"SUCCESS! New submission file 'pca_model_submission.csv' is ready in your Drive!\")\n"
      ]
    }
  ]
}